# -*- coding: utf-8 -*-
"""QuarticSmoothingSpline_GRB_1a.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ldTgf_kfvZCd9OCFtg3gaKkhec4P30_Z

-Krishnanjan Sil

GitHub Profile: https://github.com/Krishnanjan-Sil
"""

# Commented out IPython magic to ensure Python compatibility.
#Quartic Smoothing Spline code:
#-Krishnanjan Sil
#GitHub Profile: https://github.com/Krishnanjan-Sil
# Import required libraries for data processing, numerical operations, plotting, and machine learning
import os  # For file and directory operations
import numpy as np  # For numerical computations and array handling
import pandas as pd  # For data manipulation and CSV handling
import math  # For mathematical constants and functions
import matplotlib.pyplot as plt  # For creating plots
# %matplotlib inline
# Ensures plots are displayed inline in Jupyter/Colab notebooks

# Import specific modules from scikit-learn for preprocessing and cross-validation
from sklearn import preprocessing  # For data normalization (though not directly used here)
from sklearn.model_selection import KFold  # For splitting data into training/validation sets for cross-validation
from tqdm import tqdm  # For displaying progress bars (not used in this code, but imported)
from scipy import stats as st  # For statistical distributions and fitting
from scipy.sparse import issparse  # For checking if data is in sparse format (not used here)
from scipy.interpolate import UnivariateSpline  # For fitting cubic smoothing splines
import torch  # For tensor operations, used in cross-validation

# Mount Google Drive to access input files and save outputs
from google.colab import drive
drive.mount('/content/drive/')  # Mounts Google Drive to the '/content/drive/' directory in Colab

# Install the pygam library (though not used in this code, likely a leftover import)
!pip install pygam

# Define the base folder path for saving outputs (figures, CSVs, and MSE data)
folder_path = "/content/drive/MyDrive/QSP_test_1a/Saved_Outputs/"
# Create directories if they don't exist; exist_ok=True prevents errors if directories already exist
os.makedirs(folder_path, exist_ok=True)  # Base output directory
os.makedirs(f"{folder_path}/Figures", exist_ok=True)  # Directory for saving plots
os.makedirs(f"{folder_path}/CSV_data", exist_ok=True)  # Directory for saving combined CSV data
os.makedirs(f"{folder_path}/MSE_data", exist_ok=True)  # Directory for saving cross-validation MSE results
# Define the main function to process a GRB dataset and perform spline fitting, reconstruction, and cross-validation
def run_model(grb_name):
    """
    Reads a trimmed CSV for a given GRB, takes log(time) & log(flux),
    fits a cubic smoothing spline via scipy.interpolate.UnivariateSpline,
    reconstructs the light curve (adding noise using residual σ),
    plots original vs. reconstructed (with updated vertical error bars),
    saves figure + CSV, and finally runs 5‐fold CV to record train/validation MSE.
    Parameters:
        grb_name (str): Name of the GRB (e.g., 'GRB200306C') used to locate the input CSV file
    """
    # Print a header to indicate which GRB is being processed
    print(f"\n----- TRAINING FOR {grb_name} -----\n")

    # (1) Load trimmed data -----------------------------------------------------
    # Construct the file path to the trimmed CSV file for the specified GRB
    data_path = (
        f"/content/drive/MyDrive/GRB Project 1/GRB Files_shared/GRBs_trimmed/"
        f"{grb_name}_trimmed.csv"
    )
    # Read the CSV file into a pandas DataFrame
    trimmed_data = pd.read_csv(data_path)
    # Check the number of columns to assign appropriate column names
    if len(trimmed_data.columns) == 6:
        # If 6 columns, assume standard format: time, positive/negative time errors, flux, positive/negative flux errors
        trimmed_data.columns = ["t", "pos_t_err", "neg_t_err", "flux", "pos_flux_err", "neg_flux_err"]
    else:
        # If more columns (e.g., 7), assume an extra unused column (likely an index or metadata)
        trimmed_data.columns = ["0", "t", "pos_t_err", "neg_t_err", "flux", "pos_flux_err", "neg_flux_err"]

    # Sort the DataFrame by the time column to ensure data points are in increasing time order
    time_col = "t" if "t" in trimmed_data.columns else "time_sec"  # Handle different time column names
    trimmed_data = trimmed_data.sort_values(by=time_col)  # Sort by time column

    # Extract time and flux data as NumPy arrays for numerical processing
    if "t" in trimmed_data.columns:
        ts = trimmed_data["t"].to_numpy()  # Extract time values
        trim_t_val = trimmed_data["t"]  # Keep time column for later use
    else:
        ts = trimmed_data["time_sec"].to_numpy()  # Extract time values (alternative column name)
        trim_t_val = trimmed_data["time_sec"]  # Keep time column
    fluxes = trimmed_data["flux"].to_numpy()  # Extract flux values

    # Extract positive and negative time errors (in linear scale)
    positive_ts_err = (
        trimmed_data["pos_t_err"].to_numpy()  # Positive time error
        if "pos_t_err" in trimmed_data.columns
        else trimmed_data["timePos_sec"].to_numpy()  # Alternative column name
    )
    negative_ts_err = (
        trimmed_data["neg_t_err"].to_numpy()  # Negative time error
        if "neg_t_err" in trimmed_data.columns
        else trimmed_data["timeNeg_sec"].to_numpy()  # Alternative column name
    )

    # Extract positive and negative flux errors (in linear scale)
    positive_fluxes_err = (
        trimmed_data["pos_flux_err"].to_numpy()  # Positive flux error
        if "pos_flux_err" in trimmed_data.columns
        else trimmed_data["flux_errPos"].to_numpy()  # Alternative column name
    )
    negative_fluxes_err = (
        trimmed_data["neg_flux_err"].to_numpy()  # Negative flux error
        if "neg_flux_err" in trimmed_data.columns
        else trimmed_data["flux_errNeg"].to_numpy()  # Alternative column name
    )

    # (2) Compute log‐space values for observed points
    # Convert time and flux to logarithmic scale for spline fitting
    log_ts = np.log10(ts)  # Log10 of time values
    log_fluxes = np.log10(fluxes)  # Log10 of flux values

    # Compute upper and lower log-flux errors based on linear-scale errors
    pos_fluxes = fluxes + positive_fluxes_err  # Upper flux bound (flux + positive error)
    neg_fluxes = fluxes + negative_fluxes_err  # Lower flux bound (flux + negative error)
    pos_log_fluxes = np.log10(pos_fluxes)  # Log10 of upper flux bound
    neg_log_fluxes = np.log10(neg_fluxes)  # Log10 of lower flux bound
    lower_err_log = log_fluxes - neg_log_fluxes  # Lower error in log-flux (difference from mean)
    upper_err_log = pos_log_fluxes - log_fluxes  # Upper error in log-flux (difference from mean)

    # Compute log-scale time and flux errors for synthetic sampling later
    ts_err = (positive_ts_err - negative_ts_err) / 2.0  # Average time error in linear scale
    flux_err = (positive_fluxes_err - negative_fluxes_err) / 2.0  # Average flux error in linear scale
    log_ts_err = ts_err / (ts * np.log(10))  # Convert time error to log scale using derivative of log10
    log_flux_err = flux_err / (fluxes * np.log(10))  # Convert flux error to log scale

    # (3) Quick plot of observed (trimmed) data alone with updated error bars
    plt.figure(figsize=(6, 4))  # Create a new figure with size 6x4 inches
    plt.errorbar(
        log_ts,  # X-axis: log(time)
        log_fluxes,  # Y-axis: log(flux)
        yerr=[np.abs(lower_err_log), np.abs(upper_err_log)],  # Error bars for log(flux)
        fmt='o',  # Plot points as circles
        capsize=5,  # Length of error bar caps
        label="Trimmed Data"  # Legend label
    )
    plt.xlabel("log(Time)")  # X-axis label
    plt.ylabel("log(Flux)")  # Y-axis label
    plt.title(f"Original GRB: {grb_name}")  # Plot title with GRB name
    plt.legend()  # Display legend
    plt.show()  # Display the plot
    print("\n----- ORIGINAL GRB (WITH UPDATED ERROR BARS) PLOTTED ABOVE -----\n")  # Confirmation message

    # (4) Normalize log‐data for spline fitting ---------------------------------
    # Compute mean and standard deviation for normalization
    log_ts_mean = np.mean(log_ts, keepdims=True)  # Mean of log(time), keepdims for broadcasting
    log_ts_std = np.std(log_ts, keepdims=True)  # Standard deviation of log(time)
    log_flux_mean = np.mean(log_fluxes, keepdims=True)  # Mean of log(flux)
    log_flux_std = np.std(log_fluxes, keepdims=True)  # Standard deviation of log(flux)

    # Normalize log-time and log-flux data to zero mean and unit variance
    log_ts_norm = (log_ts - log_ts_mean) / log_ts_std  # Normalized log(time)
    log_flux_norm = (log_fluxes - log_flux_mean) / log_flux_std  # Normalized log(flux)

    # Convert normalized data to PyTorch tensors for cross-validation
    train_x = torch.tensor(log_ts_norm.reshape(-1, 1), dtype=torch.float32)  # Reshape to (N,1) tensor
    train_y = torch.tensor(log_flux_norm, dtype=torch.float32)  # 1D tensor for flux

    # (5) Build a dense grid for reconstruction: geom. spaced in linear‐t
    density_factor = 1  # Unused variable (likely a placeholder for future use)
    # GAP-AWARE reconstruction: create a grid that accounts for gaps in log(time)
    gaps = np.diff(log_ts)  # Compute differences between consecutive log(time) points

    # Define minimum gap size and initialize reconstruction grid
    min_gap = 0.05  # Minimum gap size in log(time) to trigger additional points
    recon_log_t = [log_ts[0]]  # Start reconstruction grid with first log(time) point
    total_span = log_ts[-1] - log_ts[0]  # Total range of log(time)

    # Determine number of reconstruction points based on dataset size
    if len(ts) > 500:  # For very dense light curves
        fraction = 0.05
    elif len(ts) > 250:
        fraction = 0.1
    elif len(ts) > 100:
        fraction = 0.3
    else:  # For sparse light curves
        fraction = 0.4
    n_points = max(20, int(fraction * len(ts)))  # Minimum 20 points, scaled by dataset size

    # Loop through gaps to add points in large gaps
    for i in range(len(ts) - 1):
        gap_size = log_ts[i+1] - log_ts[i]  # Size of gap between consecutive points
        if gap_size > min_gap:
            # Allocate points proportional to gap size
            interval_points = max(2, int(n_points * gap_size / total_span))  # At least 2 points
            interval = np.linspace(log_ts[i], log_ts[i+1], interval_points, endpoint=True)  # Linearly spaced points
            recon_log_t.extend(interval[1:])  # Add points, excluding the start to avoid duplication

    recon_log_t = np.array(recon_log_t)  # Convert to NumPy array
    recon_t = 10**np.array(recon_log_t)  # Convert back to linear time
    recon_t = np.unique(recon_t)  # Remove duplicates to ensure unique time points
    log_recon_t = np.log10(recon_t).reshape(-1, 1)  # Convert back to log(time) for spline evaluation

    # -------------------------------------------------------------------------------
    # Sub‐function: train_spline() using UnivariateSpline with the new plot format
    # -------------------------------------------------------------------------------
    def train_spline():
        print("\n----- TRAINING QUARTIC SMOOTHING SPLINE (SciPy) -----\n")

        # 1) Fit UnivariateSpline on normalized log‐data
        N = len(log_ts_norm)  # Number of data points
        spline = UnivariateSpline(
            x=log_ts_norm.flatten(),  # Normalized log(time) as input
            y=log_flux_norm.flatten(),  # Normalized log(flux) as output
            k=4,  # Quartic spline (degree 4)
            s=N  # Smoothing factor equal to number of points for balanced fit
        )

        # 2) Compute residuals in normalized space and estimate standard deviation
        pred_norm_train = spline(log_ts_norm.flatten())  # Spline predictions on training data
        resid_norm = log_flux_norm.flatten() - pred_norm_train  # Residuals (errors)
        sigma_resid = np.std(resid_norm)  # Standard deviation of residuals

        # 3) Expand the log‐grid by inserting extra points where gaps ≥ 0.1
        expanded = log_recon_t.copy()  # Start with reconstruction grid
        for i in range(len(log_ts) - 1):
            lowb = log_ts[i]  # Lower bound of gap
            upb = log_ts[i + 1]  # Upper bound of gap
            if np.abs(upb - lowb) >= 0.1:  # If gap is large enough
                n_pts = min(5, int(5 * np.abs(upb - lowb) / 0.1))  # Number of points proportional to gap size
                segment = np.linspace(lowb, upb, num=n_pts).reshape(-1, 1)  # Create points in gap
                expanded = np.vstack((expanded, segment))  # Add to expanded grid
        expanded = np.sort(expanded, axis=0)  # Sort to ensure increasing order

        # 4) Normalize the expanded grid for spline evaluation
        expanded_norm = ((expanded - log_ts_mean) / log_ts_std).flatten()  # Normalize log(time) grid

        # 5) Evaluate the spline on the normalized expanded grid
        mean_norm_recon = spline(expanded_norm)  # Predicted normalized log(flux) values

        # 6) Build a 95% confidence interval in normalized space
        lower_norm_recon = mean_norm_recon - 1.96 * sigma_resid  # Lower bound (mean - 1.96 * std)
        upper_norm_recon = mean_norm_recon + 1.96 * sigma_resid  # Upper bound (mean + 1.96 * std)

        # 7) Denormalize predictions back to log‐flux space
        mean_denorm_log = (mean_norm_recon * log_flux_std) + log_flux_mean  # Denormalized mean
        lower_denorm_log = (lower_norm_recon * log_flux_std) + log_flux_mean  # Denormalized lower bound
        upper_denorm_log = (upper_norm_recon * log_flux_std) + log_flux_mean  # Denormalized upper bound

        # 8) Generate one random‐noise realization of log‐flux with best fit noise
        # Compute flux errors in linear scale
        fluxes_error = (positive_fluxes_err - negative_fluxes_err) / 2  # Average flux error
        logfluxerrs = fluxes_error / (fluxes * np.log(10))  # Convert to log-scale flux errors

        # Fit normal and Laplace distributions to log-flux errors
        distributions = [st.norm, st.laplace]  # List of distributions to try
        fits = {}
        for dist in distributions:
            params = dist.fit(logfluxerrs)  # Fit distribution to log-flux errors
            loglikelihood = np.sum(dist.logpdf(logfluxerrs, *params))  # Compute log-likelihood
            fits[dist.name] = (params, loglikelihood)  # Store parameters and log-likelihood

        # Select the best-fitting distribution based on log-likelihood
        best_dist_name = max(fits, key=lambda d: fits[d][1])  # Distribution with highest log-likelihood
        best_params = fits[best_dist_name][0]  # Parameters of best distribution
        best_dist = getattr(st, best_dist_name)  # Get the distribution object

        # Generate random noise for each reconstructed point
        rand_noise = []
        for j in range(len(mean_norm_recon)):
            # Sample noise from the best distribution, scaled by 3.5 and centered
            noise = 3.5 * (best_dist.rvs(*best_params, size=1)[0] - best_params[0])
            rand_noise.append(noise)

        rand_noise = np.array(rand_noise)  # Convert noise list to array
        recon_norm_flux = mean_norm_recon + rand_noise  # Add noise to normalized predictions
        recon_denorm_log = (recon_norm_flux * log_flux_std) + log_flux_mean  # Denormalize noisy predictions

        # 9) Fit distributions of original log‐errors (flux & time) to sample synthetic errors
        loc_f, scale_f = st.norm.fit(log_flux_err)  # Fit normal distribution to log-flux errors
        dist_f = st.norm(loc=loc_f, scale=scale_f)  # Create normal distribution object
        sampled_flux_errs = dist_f.rvs(size=len(expanded))  # Sample flux errors for reconstructed points

        loc_t, scale_t = st.norm.fit(log_ts_err)  # Fit normal distribution to log-time errors
        dist_t = st.norm(loc=loc_t, scale=scale_t)  # Create normal distribution object
        sampled_time_errs = dist_t.rvs(size=len(expanded))  # Sample time errors for reconstructed points

        # 10) Plot original + reconstructed data with updated vertical error bars
        # Denormalize training points for plotting
        train_x_denorm = (train_x.numpy().flatten() * log_ts_std) + log_ts_mean  # Denormalized log(time)
        train_y_denorm = (train_y.numpy().flatten() * log_flux_std) + log_flux_mean  # Denormalized log(flux)

        # Prepare arrays for reconstructed data
        test_x_denorm = expanded.flatten()  # Denormalized log(time) grid for reconstruction
        log_reconstructed_flux = recon_denorm_log.flatten()  # Noisy reconstructed log(flux)
        mean_prediction_denorm = mean_denorm_log.flatten()  # Mean predicted log(flux)
        lower_denorm = lower_denorm_log.flatten()  # Lower CI bound
        upper_denorm = upper_denorm_log.flatten()  # Upper CI bound
        recon_errorbar = sampled_flux_errs  # Synthetic log-flux errors for error bars

        plt.figure(figsize=(8, 6))  # Create a new figure with size 8x6 inches

        # a) Plot original data with updated y-errors
        plt.errorbar(
            train_x_denorm,  # Log(time)
            train_y_denorm,  # Log(flux)
            zorder=4,  # Plot on top (higher z-order)
            yerr=[lower_err_log, upper_err_log],  # Error bars for log(flux)
            linestyle=""  # No connecting lines
        )

        # b) Plot reconstructed points with synthetic error bars
        plt.errorbar(
            test_x_denorm,  # Reconstructed log(time)
            log_reconstructed_flux,  # Noisy reconstructed log(flux)
            linestyle='none',  # No connecting lines
            yerr=np.abs(recon_errorbar),  # Synthetic error bars
            marker='o',  # Circle markers
            capsize=5,  # Error bar cap length
            color='yellow',  # Yellow color for reconstructed points
            zorder=3,  # Slightly lower z-order
            label="Reconstructed Points"  # Legend label
        )

        # c) Scatter original observed points on top
        plt.scatter(
            train_x_denorm,  # Log(time)
            train_y_denorm,  # Log(flux)
            zorder=5,  # Highest z-order to ensure visibility
            label="Observed Points"  # Legend label
        )

        # d) Plot the mean prediction curve
        plt.plot(
            test_x_denorm,  # Log(time)
            mean_prediction_denorm,  # Mean predicted log(flux)
            label="Mean Prediction",  # Legend label
            zorder=2  # Lower z-order
        )

        # e) Add 95% confidence interval shading
        plt.fill_between(
            test_x_denorm.flatten(),  # Log(time)
            lower_denorm,  # Lower CI bound
            upper_denorm,  # Upper CI bound
            alpha=0.5,  # Transparency
            color='orange',  # Orange color for CI
            label="95% Confidence Region",  # Legend label
            zorder=1  # Lowest z-order
        )

        plt.legend(loc='lower left')  # Place legend in lower-left corner
        plt.xlabel('log$_{10}$(Time) (s)', fontsize=15)  # X-axis label with LaTeX formatting
        plt.ylabel('log$_{10}$(Flux) ($erg\\,cm^{-2}\\,s^{-1}$)', fontsize=15)  # Y-axis label with LaTeX
        plt.title(f'Quartic Smoothing Spline on {grb_name}', fontsize=18)  # Plot title
        plt.savefig(f"{folder_path}/Figures/{grb_name}.png", dpi=300)  # Save plot as PNG
        print("\n----- RECONSTRUCTED GRB (Figure saved) -----\n")  # Confirmation message
        plt.show()  # Display the plot

        # 11) Build combined DataFrame with original and reconstructed data
        combined_df = trimmed_data.copy(deep=True)  # Deep copy of original DataFrame
        new_rows = []  # List to store reconstructed data rows
        for i in range(len(expanded)):
            logt_pt = expanded[i][0]  # Log(time) for reconstructed point
            t_lin = 10 ** logt_pt  # Convert to linear time

            # Convert log-time errors to linear scale for positive/negative errors
            pos_t_lin = 10 ** (logt_pt + sampled_time_errs[i])  # Upper time bound
            neg_t_lin = 10 ** (logt_pt - sampled_time_errs[i])  # Lower time bound

            flux_lin = 10 ** recon_denorm_log[i]  # Convert reconstructed log(flux) to linear scale
            pos_f_lin = 10 ** (recon_denorm_log[i] + sampled_flux_errs[i])  # Upper flux bound
            neg_f_lin = 10 ** (recon_denorm_log[i] - sampled_flux_errs[i])  # Lower flux bound

            # Create a dictionary for the new row
            new_rows.append({
                "t": t_lin,  # Linear time
                "pos_t_err": abs(pos_t_lin - t_lin),  # Positive time error
                "neg_t_err": abs(t_lin - neg_t_lin),  # Negative time error
                "flux": flux_lin,  # Linear flux
                "pos_flux_err": abs(pos_f_lin - flux_lin),  # Positive flux error
                "neg_flux_err": abs(flux_lin - neg_f_lin)  # Negative flux error
            })

        new_df = pd.DataFrame(new_rows)  # Create DataFrame from reconstructed rows
        combined_df = pd.concat([combined_df, new_df], ignore_index=True)  # Append reconstructed rows
        combined_df.to_csv(f"{folder_path}/CSV_data/{grb_name}.csv", index=False)  # Save combined DataFrame
        print(f"Saved combined CSV to: {folder_path}/CSV_data/{grb_name}.csv")  # Confirmation message

    # Call the spline-training sub-function
    train_spline()

    # # -------------------------------------------------------------------------------
    # # (6) 5‐Fold Cross‐Validation (on normalized log‐data) to record MSE
    # # -------------------------------------------------------------------------------
    print("\n----- 5‐FOLD VALIDATION BEGIN -----\n")

    # Convert PyTorch tensors back to NumPy arrays for cross-validation
    all_x = train_x.numpy().astype(float)  # Normalized log(time), shape (N, 1)
    all_y = train_y.numpy().astype(float)  # Normalized log(flux), shape (N,)

    # Initialize KFold with 5 splits, shuffling data, and a fixed random seed for reproducibility
    kf = KFold(n_splits=5, shuffle=True, random_state=42)
    mse_records = []  # List to store MSE for each fold

    # Iterate through folds
    for fold, (train_idx, test_idx) in enumerate(kf.split(all_x, all_y), start=1):
        print(f"--- Fold {fold} / 5 ---")  # Print fold number

        # Split data into training and validation sets
        x_tr = all_x[train_idx].reshape(-1, 1)  # Training log(time)
        y_tr = all_y[train_idx]  # Training log(flux)
        x_te = all_x[test_idx].reshape(-1, 1)  # Validation log(time)
        y_te = all_y[test_idx]  # Validation log(flux)

        # Fit a new spline on the training set
        spline_cv = UnivariateSpline(
            x=x_tr.flatten(),  # Normalized training log(time)
            y=y_tr.flatten(),  # Normalized training log(flux)
            k=4,  # Quartic spline
            s=len(x_tr)  # Smoothing factor equal to number of training points
        )

        # Compute predictions for training and validation sets
        pred_tr = spline_cv(x_tr.flatten())  # Training predictions (normalized)
        pred_tr_denorm = (pred_tr * log_flux_std) + log_flux_mean  # Denormalized training predictions
        truth_tr_denorm = (y_tr * log_flux_std) + log_flux_mean  # Denormalized training true values
        mse_tr = np.mean((pred_tr_denorm - truth_tr_denorm) ** 2)  # Training MSE in log-flux space

        pred_te = spline_cv(x_te.flatten())  # Validation predictions (normalized)
        pred_te_denorm = (pred_te * log_flux_std) + log_flux_mean  # Denormalized validation predictions
        truth_te_denorm = (y_te * log_flux_std) + log_flux_mean  # Denormalized validation true values
        mse_te = np.mean((pred_te_denorm - truth_te_denorm) ** 2)  # Validation MSE in log-flux space

        # Print MSE for the current fold
        print(
            f"Fold {fold} → Train MSE (log‐flux): {mse_tr:.4e}  |  Val MSE (log‐flux): {mse_te:.4e}"
        )
        mse_records.append((mse_tr, mse_te))  # Store MSE values

    # Compute mean MSE across all folds
    mse_arr = np.array(mse_records)  # Shape (5, 2) for train and validation MSE
    mean_tr_mse = np.mean(mse_arr[:, 0])  # Mean training MSE
    mean_te_mse = np.mean(mse_arr[:, 1])  # Mean validation MSE

    # Create a DataFrame to store MSE results
    mse_df = pd.DataFrame({
        "GRB_Name": [grb_name],  # GRB name
        "Mean_Train_MSE": [mean_tr_mse],  # Mean training MSE
        "Mean_Val_MSE": [mean_te_mse]  # Mean validation MSE
    })
    mse_csv_path = f"{folder_path}/MSE_data/MSE_data.csv"  # Path to save MSE results
    write_header = not os.path.exists(mse_csv_path)  # Write header only if file doesn't exist
    mse_df.to_csv(mse_csv_path, mode='a', header=write_header, index=False)  # Append to CSV

    # Print final cross-validation results
    print("\n----- 5‐FOLD VALIDATION COMPLETE -----\n")
    print(f"Overall Mean Train MSE: {mean_tr_mse:.4e}")
    print(f"Overall Mean Val   MSE: {mean_te_mse:.4e}")
GRBs = [
    'GRB050128', 'GRB050502B', 'GRB050607', 'GRB050712', 'GRB050713A']
#Add more ...
for grb_name in GRBs:
    run_model(grb_name)  # Process each GRB
