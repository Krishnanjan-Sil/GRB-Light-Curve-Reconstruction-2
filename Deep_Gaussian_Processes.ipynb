{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Author: Khushi Gupta  "
      ],
      "metadata": {
        "id": "r8Ii1jj_iKlC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GitHub Profile: https://github.com/iskhushii"
      ],
      "metadata": {
        "id": "PwO-ztQaklz-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0IGXtN3I-Sm"
      },
      "outputs": [],
      "source": [
        "#Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from sklearn import preprocessing\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import KFold"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "Q1kFOuaDgJvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gpytorch"
      ],
      "metadata": {
        "id": "48kfjrbliAzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gpytorch"
      ],
      "metadata": {
        "id": "0ASfd1Q0MUP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create output directory\n",
        "import os\n",
        "folder_path = f\"/content/drive/MyDrive/Deep_GP/Saved_Outputs/\"\n",
        "os.makedirs(folder_path, exist_ok=True)\n",
        "os.makedirs(f\"{folder_path}/Figures\", exist_ok=True)\n",
        "os.makedirs(f\"{folder_path}/CSV_data\", exist_ok=True)\n",
        "os.makedirs(f\"{folder_path}/MSE_data\", exist_ok=True)"
      ],
      "metadata": {
        "id": "bPKxKlxBVCxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MODEL CODE\n",
        "\n",
        "def run_model(grb_name):\n",
        "    # Start training for a given GRB\n",
        "    print(f\"\\n-----TRAINING FOR {grb_name}-----\\n\")\n",
        "\n",
        "    from scipy import stats as st\n",
        "    from gpytorch.constraints import Interval\n",
        "\n",
        "    GRB_Name = grb_name\n",
        "\n",
        "    # Load pre-trimmed GRB data\n",
        "    trimmed_data = pd.read_csv(f\"/content/drive/MyDrive/GRB_Project/GRBs_trimmed/{GRB_Name}_trimmed.csv\")\n",
        "\n",
        "    # Standardize column names depending on dataset format\n",
        "    if len(trimmed_data.columns) == 6:\n",
        "        trimmed_data.columns = [\"t\", \"pos_t_err\", \"neg_t_err\", \"flux\", \"pos_flux_err\", \"neg_flux_err\"]\n",
        "    else:\n",
        "        trimmed_data.columns = [\"0\", \"t\", \"pos_t_err\", \"neg_t_err\", \"flux\", \"pos_flux_err\", \"neg_flux_err\"]\n",
        "\n",
        "    # Helper values\n",
        "    density_factor = 1\n",
        "    max_fluxes = np.max(trimmed_data[\"flux\"])\n",
        "    min_fluxes = np.min(trimmed_data[\"flux\"])\n",
        "\n",
        "    # Time values\n",
        "    trim_t_val = trimmed_data[\"t\"] if \"t\" in trimmed_data else trimmed_data[\"time_sec\"]\n",
        "    max_ts = np.max(trim_t_val)\n",
        "    min_ts = np.min(trim_t_val)\n",
        "\n",
        "    # Convert flux + time to log-scale\n",
        "    log_max_fluxes = np.log10(max_fluxes)\n",
        "    log_min_fluxes = np.log10(min_fluxes)\n",
        "    log_max_ts = np.log10(max_ts)\n",
        "    log_min_ts = np.log10(min_ts)\n",
        "\n",
        "    # Extract symmetric errorbars in linear scale\n",
        "    positive_ts_err = trimmed_data[\"pos_t_err\"]\n",
        "    negative_ts_err = trimmed_data[\"neg_t_err\"]\n",
        "    positive_fluxes_err = trimmed_data[\"pos_flux_err\"]\n",
        "    negative_fluxes_err = trimmed_data[\"neg_flux_err\"]\n",
        "\n",
        "    ts = trim_t_val.to_numpy()\n",
        "    fluxes = trimmed_data[\"flux\"].to_numpy()\n",
        "\n",
        "    # Convert values to log-scale\n",
        "    log_ts = np.log10(ts)\n",
        "    log_fluxes = np.log10(fluxes)\n",
        "\n",
        "    # Build flux errorbars in linear scale\n",
        "    pos_fluxes = fluxes + positive_fluxes_err\n",
        "    neg_fluxes = fluxes + negative_fluxes_err\n",
        "\n",
        "    fluxes_err_sym = (pos_fluxes - neg_fluxes) / 2\n",
        "\n",
        "    # Generate reconstruction time points using geometric spacing\n",
        "    recon_t = np.geomspace(np.min(ts), np.max(ts), density_factor * len(ts))\n",
        "    log_recon_t = np.log10(recon_t).reshape(-1, 1)\n",
        "\n",
        "    # Compute time + flux errors\n",
        "    ts_error = (positive_ts_err - negative_ts_err) / 2\n",
        "    fluxes_error = (positive_fluxes_err - negative_fluxes_err) / 2\n",
        "\n",
        "    pos_log_fluxes = np.log10(pos_fluxes)\n",
        "    neg_log_fluxes = np.log10(neg_fluxes)\n",
        "\n",
        "    # PLOT ORIGINAL GRB\n",
        "    plt.xlabel(\"Time\")\n",
        "    plt.ylabel(\"Flux\")\n",
        "    plt.errorbar(log_ts, log_fluxes, linestyle='none', yerr=[log_fluxes - neg_log_fluxes, pos_log_fluxes - log_fluxes], marker='o', capsize=5, label=\"Trimmed Data\")\n",
        "    plt.title(GRB_Name)\n",
        "    print(\"\\n-----ORIGINAL GRB-----\\n\")\n",
        "    plt.show()\n",
        "\n",
        "    # ------------------------------------------\n",
        "    # NORMALIZATION FOR STABLE DGP TRAINING\n",
        "    # ------------------------------------------\n",
        "    log_ts_mean = np.mean(log_ts, keepdims=True)\n",
        "    log_ts_std = np.std(log_ts, keepdims=True)\n",
        "    log_ts_norm = (log_ts - log_ts_mean) / log_ts_std\n",
        "\n",
        "    log_fluxes_mean = np.mean(log_fluxes, keepdims=True)\n",
        "    log_fluxes_std = np.std(log_fluxes, keepdims=True)\n",
        "    log_fluxes_norm = (log_fluxes - log_fluxes_mean) / log_fluxes_std\n",
        "\n",
        "    # Convert to torch tensors\n",
        "    train_x = torch.tensor(log_ts_norm, dtype=torch.float32)\n",
        "    train_y = torch.tensor(log_fluxes_norm, dtype=torch.float32)\n",
        "    log_fluxes_error = torch.tensor(fluxes_error / fluxes, dtype=torch.float32)\n",
        "\n",
        "    # ---------------------------------------------------\n",
        "    # SINGLE GP LAYER USED INSIDE THE DEEP GP\n",
        "    # ---------------------------------------------------\n",
        "    class SingleLayerGP(gpytorch.models.ApproximateGP):\n",
        "        def __init__(self, input_dim, inducing_points):\n",
        "            inducing_points = train_x[:inducing_points]\n",
        "\n",
        "            variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(\n",
        "                inducing_points.size(0)\n",
        "            )\n",
        "            variational_strategy = gpytorch.variational.VariationalStrategy(\n",
        "                self, inducing_points, variational_distribution, learn_inducing_locations=True\n",
        "            )\n",
        "\n",
        "            super().__init__(variational_strategy)\n",
        "\n",
        "            self.mean_module = gpytorch.means.ConstantMean()\n",
        "            self.covar_module = gpytorch.kernels.ScaleKernel(\n",
        "                gpytorch.kernels.RBFKernel(lengthscale=2.0, lengthscale_bounds=(0.05, 10.0))\n",
        "            )\n",
        "\n",
        "        def forward(self, x):\n",
        "            mean_x = self.mean_module(x)\n",
        "            covar_x = self.covar_module(x)\n",
        "            return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
        "\n",
        "    # ---------------------------------------------------\n",
        "    # THE DEEP GP MODEL = TWO GP LAYERS STACKED\n",
        "    # ---------------------------------------------------\n",
        "    class DeepGP(torch.nn.Module):\n",
        "        def __init__(self, num_inducing):\n",
        "            super().__init__()\n",
        "\n",
        "            # Two-level GP hierarchy\n",
        "            self.hidden_layer = SingleLayerGP(train_x.shape[-1], inducing_points=35)\n",
        "            self.final_layer = SingleLayerGP(train_x.shape[-1], inducing_points=25)\n",
        "\n",
        "            # Gaussian likelihood with constrained noise\n",
        "            self.likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
        "\n",
        "            self.likelihood.noise_covar.register_constraint(\"raw_noise\", Interval(1e-4, 0.05))\n",
        "            self.likelihood.noise_covar.initialize(noise=0.01)\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = self.hidden_layer(x)\n",
        "            x = self.final_layer(x.mean)\n",
        "            return x\n",
        "\n",
        "    # ---------------------------------------------------\n",
        "    # TRAINING FUNCTION FOR DEEP GP\n",
        "    # ---------------------------------------------------\n",
        "    from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "    def train_dgp():\n",
        "        print(\"\\n-----TRAINING DEEP GP-----\\n\")\n",
        "\n",
        "        model = DeepGP(num_inducing=50)\n",
        "        likelihood = model.likelihood\n",
        "\n",
        "        model.train()\n",
        "        likelihood.train()\n",
        "\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
        "        scheduler = StepLR(optimizer, step_size=200, gamma=0.5)\n",
        "\n",
        "        mll = gpytorch.mlls.VariationalELBO(likelihood, model.final_layer, num_data=train_y.size(0))\n",
        "\n",
        "        num_iterations = 500\n",
        "\n",
        "        for i in range(num_iterations):\n",
        "            optimizer.zero_grad()\n",
        "            output = model(train_x)\n",
        "            loss = -mll(output, train_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            if (i + 1) % 20 == 0:\n",
        "                print(f\"Iteration {i + 1}/{num_iterations} - Loss: {loss.item():.4f}\")\n",
        "\n",
        "        model.eval()\n",
        "        likelihood.eval()\n",
        "\n",
        "        # ---------------------------------------------------\n",
        "        # GAP-AWARE RECONSTRUCTION\n",
        "        # ---------------------------------------------------\n",
        "        with torch.no_grad():\n",
        "            gaps = np.diff(log_ts)\n",
        "\n",
        "            # Minimum allowed gap size in log-time\n",
        "            min_gap = 0.05\n",
        "            recon_log_t = [log_ts[0]]\n",
        "            total_span = log_ts[-1] - log_ts[0]\n",
        "\n",
        "            # Density allocation rules depending on dataset size\n",
        "            if len(ts) > 500:\n",
        "                fraction = 0.05\n",
        "            elif len(ts) > 250:\n",
        "                fraction = 0.1\n",
        "            elif len(ts) > 100:\n",
        "                fraction = 0.3\n",
        "            else:\n",
        "                fraction = 0.4\n",
        "\n",
        "            n_points = max(20, int(fraction * len(ts)))\n",
        "\n",
        "            # Allocate new points depending on gap size\n",
        "            for i in range(len(ts) - 1):\n",
        "                gap_size = log_ts[i + 1] - log_ts[i]\n",
        "\n",
        "                if gap_size > min_gap:\n",
        "                    interval_points = max(2, int(n_points * gap_size / total_span))\n",
        "                    interval = np.linspace(log_ts[i], log_ts[i+1], interval_points, endpoint=True)\n",
        "                    recon_log_t.extend(interval[1:])\n",
        "\n",
        "            # Prepare test_x\n",
        "            recon_log_t = np.array(recon_log_t)\n",
        "            recon_t = 10 ** np.array(recon_log_t)\n",
        "            recon_t = np.unique(recon_t)\n",
        "\n",
        "            log_recon_t = np.log10(recon_t).reshape(-1, 1)\n",
        "            log_recon_t = np.sort(log_recon_t, axis=0)\n",
        "\n",
        "            test_x = torch.tensor(log_recon_t, dtype=torch.float32)\n",
        "            test_x = (test_x - log_ts.mean()) / log_ts.std()\n",
        "\n",
        "            observed_pred = likelihood(model(test_x))\n",
        "            std_prediction = observed_pred.stddev.numpy()\n",
        "            mean_prediction = observed_pred.mean.numpy()\n",
        "            lower, upper = observed_pred.confidence_region()\n",
        "\n",
        "        # ---------------------------------------------------\n",
        "        # DENORMALIZE PREDICTIONS\n",
        "        # ---------------------------------------------------\n",
        "        mean_prediction_denorm = (mean_prediction * log_fluxes_std) + log_fluxes_mean\n",
        "        lower_denorm = (lower.numpy() * log_fluxes_std) + log_fluxes_mean\n",
        "        upper_denorm = (upper.numpy() * log_fluxes_std) + log_fluxes_mean\n",
        "\n",
        "        # Monte Carlo sampling for flux reconstruction noise\n",
        "        points = []\n",
        "        for j in range(len(mean_prediction)):\n",
        "            fitted_dist = st.norm(loc=mean_prediction[j], scale=std_prediction[j])\n",
        "            point = np.random.choice(fitted_dist.rvs(size=50), size=1) - mean_prediction[j]\n",
        "            points.append(point)\n",
        "\n",
        "        points = np.array(points).ravel()\n",
        "        new_points = mean_prediction + points\n",
        "\n",
        "        log_reconstructed_flux = (new_points * log_fluxes_std) + log_fluxes_mean\n",
        "\n",
        "        # Denormalize train_x and train_y\n",
        "        test_x_denorm = test_x * log_ts.std() + log_ts.mean()\n",
        "        train_x_denorm = (train_x.numpy() * log_ts_std) + log_ts_mean\n",
        "        train_y_denorm = (train_y.numpy() * log_fluxes_std) + log_fluxes_mean\n",
        "\n",
        "        # Reconstruct synthetic error distributions\n",
        "        fluxes_error = (positive_fluxes_err - negative_fluxes_err) / 2\n",
        "        logfluxerrs = fluxes_error / (fluxes * np.log(10))\n",
        "\n",
        "        errparameters = st.norm.fit(logfluxerrs)\n",
        "        err_dist = st.norm(loc=errparameters[0], scale=errparameters[1])\n",
        "        recon_errorbar = err_dist.rvs(size=len(log_recon_t))\n",
        "\n",
        "        # ---------------------------------------------------\n",
        "        # PLOT RECONSTRUCTED GRB\n",
        "        # ---------------------------------------------------\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.errorbar(train_x_denorm, train_y_denorm, zorder=4,\n",
        "                      yerr=[log_fluxes - neg_log_fluxes, pos_log_fluxes - log_fluxes], linestyle=\"\")\n",
        "        plt.errorbar(test_x_denorm, log_reconstructed_flux, linestyle='none',\n",
        "                      yerr=np.abs(recon_errorbar), marker='o', capsize=5, color='yellow', zorder=3,\n",
        "                      label=\"Reconstructed Points\")\n",
        "        plt.scatter(train_x_denorm, train_y_denorm, zorder=5, label='Observed Points')\n",
        "        plt.plot(test_x_denorm, mean_prediction_denorm, label='Mean Prediction', zorder=2)\n",
        "        plt.fill_between(test_x_denorm.flatten(), lower_denorm, upper_denorm,\n",
        "                         alpha=0.5, color='orange', label='95% Confidence Interval', zorder=1)\n",
        "\n",
        "        plt.legend(loc='lower left')\n",
        "        plt.xlabel('log$_{10}$(Time) (s)', fontsize=\"15\")\n",
        "        plt.ylabel(\"log$_{10}$(Flux) ($erg$ ${cm^{-2}}$$s^{-1}$)\", fontsize=\"15\")\n",
        "        plt.title(f'Deep GP on {GRB_Name}', fontsize=\"18\")\n",
        "        plt.savefig(f\"/content/drive/MyDrive/{GRB_Name}.png\", dpi=300)\n",
        "\n",
        "        print(\"\\n-----RECONSTRUCTED GRB-----\\n\")\n",
        "        plt.show()\n",
        "\n",
        "        # Time error modeling\n",
        "        log_ts_error = ts_error / (ts * np.log(10))\n",
        "        errparameters = st.norm.fit(log_ts_error)\n",
        "        err_dist_time = st.norm(loc=errparameters[0], scale=errparameters[1])\n",
        "        recon_logtimeerr = err_dist_time.rvs(size=len(log_recon_t))\n",
        "\n",
        "        df = trimmed_data.copy(deep=True)\n",
        "\n",
        "        # Save reconstructed LC into CSV\n",
        "        for k in range(len(log_recon_t)):\n",
        "            new_row = {\n",
        "                \"t\": 10 ** log_recon_t[k],\n",
        "                \"pos_t_err\": 10 ** recon_logtimeerr[k],\n",
        "                \"neg_t_err\": 10 ** recon_logtimeerr[k],\n",
        "                \"flux\": 10 ** log_reconstructed_flux[k],\n",
        "                \"pos_flux_err\": 10 ** log_reconstructed_flux[k] * np.log(10) * recon_errorbar[k],\n",
        "                \"neg_flux_err\": 10 ** log_reconstructed_flux[k] * np.log(10) * recon_errorbar[k]\n",
        "            }\n",
        "\n",
        "            new_row_df = pd.DataFrame(new_row)\n",
        "            df = pd.concat([df, new_row_df], ignore_index=True)\n",
        "\n",
        "        df.to_csv(f\"{folder_path}/CSV_data/{GRB_Name}.csv\", index=False)\n",
        "\n",
        "    # Call the training function\n",
        "    train_dgp()\n",
        "\n",
        "    # ---------------------------------------------------------------\n",
        "    #                   5-FOLD CROSS VALIDATION\n",
        "    # ---------------------------------------------------------------\n",
        "    print(\"\\n-----5K FOLD VALIDATION BEGIN-----\\n\")\n",
        "\n",
        "    from sklearn.model_selection import KFold\n",
        "    import torch.nn.functional as F\n",
        "\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    file_path = f\"{folder_path}/MSE_data.csv\"\n",
        "    write_header = not os.path.exists(file_path)\n",
        "\n",
        "    test_mse_scores = []\n",
        "    train_mse_scores = []\n",
        "\n",
        "    # Perform 5-fold KFold training and evaluation\n",
        "    for fold, (train_idx, test_idx) in enumerate(kf.split(train_x, train_y)):\n",
        "        print(f\"Training on fold {fold + 1}/5...\")\n",
        "\n",
        "        train_x_fold, test_x_fold = train_x[train_idx].to(device), train_x[test_idx].to(device)\n",
        "        train_y_fold, test_y_fold = train_y[train_idx].to(device), train_y[test_idx].to(device)\n",
        "\n",
        "        model = DeepGP(num_inducing=50).to(device)\n",
        "        likelihood = model.likelihood.to(device)\n",
        "\n",
        "        model.train()\n",
        "        likelihood.train()\n",
        "\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
        "        mll = gpytorch.mlls.VariationalELBO(likelihood, model.final_layer, num_data=train_y_fold.size(0))\n",
        "\n",
        "        num_iterations = 200\n",
        "        for i in range(num_iterations):\n",
        "            optimizer.zero_grad()\n",
        "            output = model(train_x_fold)\n",
        "\n",
        "            train_pred = output.mean\n",
        "\n",
        "            # Denormalize predictions and targets for MSE computation\n",
        "            train_pred_denorm = (train_pred.detach().cpu().numpy() * log_fluxes_std + log_fluxes_mean)\n",
        "            train_y_denorm = (train_y_fold.detach().cpu().numpy() * log_fluxes_std + log_fluxes_mean)\n",
        "\n",
        "            train_pred_tensor = torch.tensor(train_pred_denorm, dtype=torch.float32)\n",
        "            train_y_tensor = torch.tensor(train_y_denorm, dtype=torch.float32)\n",
        "\n",
        "            loss = -mll(output, train_y_fold)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Compute train MSE for this iteration\n",
        "            train_mse_value = F.mse_loss(train_pred_tensor, train_y_tensor).item()\n",
        "            train_mse_scores.append(train_mse_value)\n",
        "\n",
        "            if (i + 1) % 50 == 0:\n",
        "                print(f\"Fold {fold + 1} - Iteration {i + 1}/{num_iterations} - Loss: {loss.item():.4f}\")\n",
        "\n",
        "        # Evaluate model\n",
        "        model.eval()\n",
        "        likelihood.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            observed_pred = likelihood(model(test_x_fold))\n",
        "            mean_prediction = observed_pred.mean\n",
        "\n",
        "            test_pred_denorm = (mean_prediction.detach().cpu().numpy() * log_fluxes_std + log_fluxes_mean)\n",
        "            test_y_fold_denorm = (test_y_fold.detach().cpu().numpy() * log_fluxes_std + log_fluxes_mean)\n",
        "\n",
        "            test_pred_tensor = torch.tensor(test_pred_denorm, dtype=torch.float32)\n",
        "            test_y_tensor = torch.tensor(test_y_fold_denorm, dtype=torch.float32)\n",
        "\n",
        "            test_mse_value = F.mse_loss(test_pred_tensor, test_y_tensor).item()\n",
        "            test_mse_scores.append(test_mse_value)\n",
        "\n",
        "        print(f\"Fold {fold + 1} - Test MSE: {test_mse_value:.4f}\")\n",
        "\n",
        "    # Compute mean metrics\n",
        "    mean_train_mse = np.mean(train_mse_scores)\n",
        "    mean_test_mse = np.mean(test_mse_scores)\n",
        "\n",
        "    # Save MSE results\n",
        "    data = {\n",
        "        \"GRB_Name\": [GRB_Name],\n",
        "        \"Final_Train_MSE\": [mean_train_mse],\n",
        "        \"Final_Validation_MSE\": [mean_test_mse]\n",
        "    }\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_csv(file_path, mode='a', header=write_header, index=False)\n",
        "\n",
        "    print(\"\\n-----SAVING TRAIN AND TEST MSE-----\\n\")\n",
        "    print(f\"Mean Train MSE: {mean_train_mse:4f}\")\n",
        "    print(f\"Mean Test MSE: {mean_test_mse:.4f}\")\n"
      ],
      "metadata": {
        "id": "NID2rXhz_WEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Add GRB names and run\n",
        "GRBs=[\"GRB060206\"]\n",
        "\n",
        "for grb_name in GRBs:\n",
        "  run_model(grb_name)"
      ],
      "metadata": {
        "id": "SQtowwP2hO4I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
