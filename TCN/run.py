# -*- coding: utf-8 -*-
"""GitHub_TCN_1a.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1M5ZtF9oDGJJNUH7jAyJ_n93ujCglXb3x

Contributors: Krishnanjan Sil, A. Manchanda

GitHub Profile: https://github.com/Krishnanjan-Sil , https://github.com/1Adi1812
"""

# Commented out IPython magic to ensure Python compatibility.
#Contributors: Krishnanjan Sil, A. Manchanda
#GitHub Profiles: https://github.com/Krishnanjan-Sil , https://github.com/1Adi1812
!pip install keras-tcn

# --- IMPORTS ---
import os
import csv
import random
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
import keras

from scipy import stats as st
from scipy.stats import norm
from sklearn.model_selection import train_test_split, KFold
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from tcn import TCN

# For notebook plotting
# %matplotlib inline

# --- SEED SETTING FOR REPRODUCIBILITY ---
seed_value = 42
random.seed(seed_value)
np.random.seed(seed_value)
tf.random.set_seed(seed_value)

# --- DIRECTORY SETUP ---
# Create necessary directories for outputs
os.makedirs('TCN_csv', exist_ok=True)
os.makedirs('TCN_Images', exist_ok=True)
os.makedirs('MSE_data', exist_ok=True) # Folder for CV results

# --- DATA SELECTION ---
# List of GRB files to process
ls = ['GRB091208B_trimmed.csv']

'''
ADD MORE GRBs:

ls = ['GRB160905A_trimmed.csv',
 'GRB150821A_trimmed.csv',
 'GRB170705A_trimmed.csv',
 'GRB181023A_trimmed.csv',
 'GRB190203A_trimmed.csv',
 'GRB190202A_trimmed.csv']
'''

for GRB_Name in ls:
    print(f"\nProcessing: {GRB_Name}")

    # --- 1. DATA LOADING & PREPROCESSING ---

    # Load the trimmed data
    trimmed_data = pd.read_csv(GRB_Name, verbose=False, skiprows=1, skip_blank_lines=True, sep=',', dtype=float, header=None)

    # Standardize column names based on file structure
    if len(trimmed_data.columns) == 6:
        trimmed_data.columns = ["t", "pos_t_err", "neg_t_err", "flux", "pos_flux_err", "neg_flux_err"]
    else:
        trimmed_data.columns = ["0", "t", "pos_t_err", "neg_t_err", "flux", "pos_flux_err", "neg_flux_err"]

    # Extract Time and Flux (Linear Scale)
    ts, fluxes = trimmed_data["t"].to_numpy(), trimmed_data["flux"].to_numpy()

    # Convert to Log Scale (features for the model)
    log_ts, log_fluxes = np.log10(ts), np.log10(fluxes)

    # Error handling (Linear Scale)
    positive_fluxes_err = trimmed_data["pos_flux_err"]
    negative_fluxes_err = trimmed_data["neg_flux_err"]
    pos_fluxes = fluxes + positive_fluxes_err
    neg_fluxes = fluxes + negative_fluxes_err

    # Error handling (Log Scale for plotting)
    pos_log_fluxes = np.log10(pos_fluxes)
    neg_log_fluxes = np.log10(neg_fluxes)

    # --- 2. GAP-AWARE TIME GENERATION ---
    # Creating dense time points in large gaps for reconstruction
    gaps = np.diff(log_ts)
    min_gap = 0.05
    recon_log_t = [log_ts[0]]
    total_span = log_ts[-1] - log_ts[0]

    # Determine density based on data length
    if len(ts) > 500: fraction = 0.05
    elif len(ts) > 250: fraction = 0.1
    elif len(ts) > 100: fraction = 0.3
    else: fraction = 0.4

    n_points = max(20, int(fraction * len(ts)))

    for i in range(len(ts) - 1):
        gap_size = log_ts[i + 1] - log_ts[i]
        if gap_size > min_gap:
            interval_points = max(2, int(n_points * gap_size / total_span))
            interval = np.linspace(log_ts[i], log_ts[i + 1], interval_points, endpoint=True)
            recon_log_t.extend(interval[1:])

    recon_log_t = np.array(recon_log_t)
    recon_t = 10 ** np.array(recon_log_t)
    recon_t = np.unique(recon_t)
    log_recon_t = np.log10(recon_t).reshape(-1, 1)

    # Plot Trimmed Data
    plt.figure()
    plt.xlabel("log(Time)")
    plt.ylabel("log(Flux)")
    plt.errorbar(log_ts, log_fluxes, linestyle='none',
                 yerr=[log_fluxes-neg_log_fluxes, pos_log_fluxes-log_fluxes],
                 marker='o', capsize=5, label="Trimmed Data")
    plt.title(f"Original Data: {GRB_Name}")
    plt.show()

    # --- 3. MODEL PREPARATION ---

    # Gaussian Fitting on Error Distribution
    fluxes_error = (positive_fluxes_err - negative_fluxes_err)/2
    logfluxerrs = fluxes_error/(fluxes*np.log(10))
    errparameters = st.norm.fit(logfluxerrs)
    err_dist = st.norm(loc=errparameters[0], scale=errparameters[1])
    recon_errorbar = err_dist.rvs(size=len(log_recon_t))

    # Prepare Features (X) and Target (y)
    X, y = log_ts.reshape(-1, 1), log_fluxes.reshape(-1, 1)
    print("Features:", X.shape, "Target:", y.shape)

    # Standard Train/Validation Split for Model Selection
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42, shuffle=True)

    # Scaling
    x_scaler = MinMaxScaler()
    y_scaler = MinMaxScaler()

    X_train_scaled = x_scaler.fit_transform(X_train).reshape(-1, 1, 1)
    y_train_scaled = y_scaler.fit_transform(y_train).reshape(-1, 1, 1)
    X_val_scaled = x_scaler.transform(X_val).reshape(-1, 1, 1)
    y_val_scaled = y_val_scaled.reshape(-1, 1, 1) # y_val is already scaled via transform? No, need to transform y_val
    y_val_scaled = y_scaler.transform(y_val).reshape(-1, 1, 1) # FIXED: added transform

    # Model Architecture Parameters
    nb_filters, kernel_size, dropout_rate = 64, 5, 0.1

    # --- 4. TCN TRAINING (Reconstruction Phase) ---
    model = keras.Sequential([
        TCN(nb_filters=nb_filters, kernel_size=kernel_size, nb_stacks=1,
            dilations=(1, 2, 4, 8, 16, 32), padding='causal',
            dropout_rate=dropout_rate, activation='relu'),
        keras.layers.Dense(1, activation='linear')
    ])

    model.compile(optimizer="adam", loss='mean_squared_error',
                  metrics=['mean_squared_error','mean_absolute_error'])

    checkpoint_filepath_final = f'{nb_filters}_{kernel_size}_{dropout_rate}_final.keras'
    model_checkpoint_callback = keras.callbacks.ModelCheckpoint(
        filepath=checkpoint_filepath_final, monitor='val_mean_squared_error',
        mode='min', save_best_only=True)

    history = model.fit(
        x=X_train_scaled, y=y_train_scaled,
        batch_size=8, epochs=100,
        validation_data=(X_val_scaled, y_val_scaled),
        shuffle=True, callbacks=[model_checkpoint_callback], verbose=0
    )

    # Plot Loss
    plt.figure()
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('Model Loss')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Val'], loc='upper left')
    plt.show()

    # --- 5. RECONSTRUCTION & JIGGLING ---

    # Load best model
    model = keras.models.load_model(checkpoint_filepath_final)

    # Predict on reconstructed time points
    log_recon_t_scaled = x_scaler.transform(log_recon_t).reshape(-1, 1, 1)
    recon_pred = model.predict(log_recon_t_scaled, verbose=0)
    predictions = y_scaler.inverse_transform(recon_pred)

    # Add Noise (Jiggling)
    ts_error = (positive_ts_err - negative_ts_err)/2
    log_ts_error = ts_error/(ts*np.log(10))
    errparameters_time = st.norm.fit(log_ts_error)
    err_dist_time = st.norm(loc=errparameters_time[0], scale=errparameters_time[1])
    recon_logtimeerr = err_dist_time.rvs(size=len(log_recon_t))

    # Generate realizations for Confidence Intervals
    num_samples = 100
    jiggled_realizations = []

    # Base point noise for the primary reconstruction
    point_specific_noise_base = []
    for j in range(len(predictions)):
        fitted_dist = norm(loc=predictions[j], scale=recon_errorbar[j])
        point_specific_noise_base.append(fitted_dist.rvs() - predictions[j])
    jiggled_points = predictions + np.array(point_specific_noise_base)

    # Multiple realizations
    for _ in range(num_samples):
        temp_noise = []
        for j in range(len(predictions)):
            fitted_dist = norm(loc=predictions[j], scale=recon_errorbar[j])
            temp_noise.append(fitted_dist.rvs() - predictions[j])
        jiggled_realizations.append(predictions + np.array(temp_noise))

    jiggled_realizations = np.array(jiggled_realizations)

    # Statistics
    mean_prediction_denorm = np.mean(jiggled_realizations, axis=0).flatten()
    ci_95_lower = np.percentile(jiggled_realizations, 2.5, axis=0).flatten()
    ci_95_upper = np.percentile(jiggled_realizations, 97.5, axis=0).flatten()

    # --- 6. FINAL PLOTTING ---

    # Flatten arrays for plotting
    log_recon_t_flat = log_recon_t.flatten()
    jiggled_points_flat = jiggled_points.flatten()
    X_flat = X.flatten()
    y_flat = y.flatten()

    plt.figure(figsize=(10, 8))

    # Reconstructed points
    plt.errorbar(log_recon_t_flat, jiggled_points_flat,
                 yerr=np.abs(recon_errorbar), linestyle='none', marker='o',
                 capsize=5, color='yellow', zorder=3, label="Reconstructed Points")

    # Original points
    plt.scatter(X_flat, y_flat, zorder=5, color='blue', label="Observed Points")

    # Mean prediction
    plt.plot(log_recon_t_flat, mean_prediction_denorm, color='green', label="Mean Prediction", zorder=2)

    # Confidence Interval
    plt.fill_between(log_recon_t_flat, ci_95_lower, ci_95_upper, alpha=0.5, color='orange', label="95% CI", zorder=1)

    plt.legend(loc='lower left')
    plt.xlabel('log$_{10}$(Time) (s)', fontsize=15)
    plt.ylabel('log$_{10}$(Flux) ($erg\\,cm^{-2}\\,s^{-1}$)', fontsize=15)
    plt.title(f'TCN Reconstruction: {GRB_Name}', fontsize=18)
    plt.savefig(f"./TCN_Images/{GRB_Name[:-4]}.png", dpi=300)
    plt.show()
    print("\n----- RECONSTRUCTION COMPLETE (Image Saved) -----\n")

    # --- 7. SAVING RECONSTRUCTED DATA ---
    df_out = trimmed_data.copy(deep=True)
    for k in range(len(log_recon_t)):
        new_row = {
            "t": 10**log_recon_t_flat[k],
            "pos_t_err": 10**recon_logtimeerr[k],
            "neg_t_err": 10**recon_logtimeerr[k],
            "flux": 10**jiggled_points_flat[k],
            "pos_flux_err": 10**jiggled_points_flat[k] * np.log(10) * recon_errorbar[k],
            "neg_flux_err": 10**jiggled_points_flat[k] * np.log(10) * recon_errorbar[k]
        }
        new_row_df = pd.DataFrame([new_row])
        df_out = pd.concat([df_out, new_row_df], ignore_index=True)

    df_out.to_csv(f'./TCN_csv/{GRB_Name[:-12]}.csv', index=False)

    # =============================================================================
    #                      5-FOLD CROSS VALIDATION (ADDED BLOCK)
    # =============================================================================
    # This block performs 5-fold CV on the ORIGINAL OBSERVED DATA (X, y) to
    # rigorously evaluate the model architecture's performance.

    print(f"\n----- 5-FOLD CROSS VALIDATION BEGINNING FOR {GRB_Name} -----\n")

    # Initialize KFold
    kf = KFold(n_splits=5, shuffle=True, random_state=42)

    # Storage for scores
    test_mse_scores = []
    train_mse_scores = []

    # File path for saving MSE results
    mse_file_path = "./MSE_data/TCN_MSE_data.csv"
    write_header = not os.path.exists(mse_file_path)

    # Iterate through folds
    for fold, (train_idx, test_idx) in enumerate(kf.split(X, y)):
        print(f"  Training on fold {fold + 1}/5...")

        # Split data for this fold
        # X and y are the original log_ts and log_fluxes
        X_train_fold, X_test_fold = X[train_idx], X[test_idx]
        y_train_fold, y_test_fold = y[train_idx], y[test_idx]

        # CRITICAL: Re-initialize Scalers for each fold to prevent data leakage.
        # We fit scaler ONLY on the training set of the current fold.
        fold_x_scaler = MinMaxScaler()
        fold_y_scaler = MinMaxScaler()

        X_train_fold_scaled = fold_x_scaler.fit_transform(X_train_fold)
        y_train_fold_scaled = fold_y_scaler.fit_transform(y_train_fold)

        # Transform test set using the scalers fitted on train set
        X_test_fold_scaled = fold_x_scaler.transform(X_test_fold)
        y_test_fold_scaled = fold_y_scaler.transform(y_test_fold)

        # Reshape inputs for TCN: (Samples, Timesteps, Features) -> (N, 1, 1)
        X_train_fold_reshaped = X_train_fold_scaled.reshape(-1, 1, 1)
        X_test_fold_reshaped = X_test_fold_scaled.reshape(-1, 1, 1)
        y_train_fold_reshaped = y_train_fold_scaled.reshape(-1, 1, 1)
        y_test_fold_reshaped = y_test_fold_scaled.reshape(-1, 1, 1)

        # Re-instantiate the Model (Fresh weights for every fold)
        # Using the same hyperparameters as the main reconstruction model
        model_cv = keras.Sequential([
            TCN(
                nb_filters=64,
                kernel_size=5,
                nb_stacks=1,
                dilations=(1, 2, 4, 8, 16, 32),
                padding='causal',
                dropout_rate=0.1,
                activation='relu'
            ),
            keras.layers.Dense(1, activation='linear')
        ])

        model_cv.compile(optimizer="adam", loss='mean_squared_error')

        # Train on the fold
        # Verbose=0 suppresses the epoch-by-epoch log to keep output clean
        model_cv.fit(
            X_train_fold_reshaped,
            y_train_fold_reshaped,
            batch_size=8,
            epochs=100,
            verbose=0
        )

        # Generate Predictions
        train_pred_scaled = model_cv.predict(X_train_fold_reshaped, verbose=0)
        test_pred_scaled = model_cv.predict(X_test_fold_reshaped, verbose=0)

        # Denormalize predictions (Back to Log Scale Flux)
        # We compare predicted Log(Flux) vs Actual Log(Flux)
        train_pred = fold_y_scaler.inverse_transform(train_pred_scaled)
        test_pred = fold_y_scaler.inverse_transform(test_pred_scaled)

        # Compute MSE
        # Comparison is between actual Log(Flux) and Predicted Log(Flux)
        train_mse = mean_squared_error(y_train_fold, train_pred)
        test_mse = mean_squared_error(y_test_fold, test_pred)

        train_mse_scores.append(train_mse)
        test_mse_scores.append(test_mse)

        print(f"    Fold {fold + 1} - Test MSE: {test_mse:.6f}")

    # Compute Mean Metrics across 5 folds
    mean_train_mse = np.mean(train_mse_scores)
    mean_test_mse = np.mean(test_mse_scores)

    # Save MSE results to CSV
    data_cv = {
        "GRB_Name": [GRB_Name],
        "Final_Train_MSE": [mean_train_mse],
        "Final_Validation_MSE": [mean_test_mse]
    }

    df_cv = pd.DataFrame(data_cv)
    df_cv.to_csv(mse_file_path, mode='a', header=write_header, index=False)

    print("\n----- SAVING TRAIN AND TEST MSE (CV) -----")
    print(f"Mean Train MSE: {mean_train_mse:.6f}")
    print(f"Mean Test MSE: {mean_test_mse:.6f}\n")
    print("-" * 60)
